{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7414349,"sourceType":"datasetVersion","datasetId":4313035},{"sourceId":7444463,"sourceType":"datasetVersion","datasetId":4333091},{"sourceId":7454820,"sourceType":"datasetVersion","datasetId":4336939},{"sourceId":7455631,"sourceType":"datasetVersion","datasetId":4339391},{"sourceId":7455732,"sourceType":"datasetVersion","datasetId":4328548},{"sourceId":4298,"sourceType":"modelInstanceVersion","modelInstanceId":3093}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Core distillation process","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-24T18:24:19.735993Z","iopub.execute_input":"2024-01-24T18:24:19.736361Z","iopub.status.idle":"2024-01-24T18:24:20.091016Z","shell.execute_reply.started":"2024-01-24T18:24:19.736331Z","shell.execute_reply":"2024-01-24T18:24:20.090227Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git \n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q datasets","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:24:20.093336Z","iopub.execute_input":"2024-01-24T18:24:20.093712Z","iopub.status.idle":"2024-01-24T18:26:33.353518Z","shell.execute_reply.started":"2024-01-24T18:24:20.093686Z","shell.execute_reply":"2024-01-24T18:26:33.352294Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install numpy==1.22.0","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:26:33.355763Z","iopub.execute_input":"2024-01-24T18:26:33.356100Z","iopub.status.idle":"2024-01-24T18:26:49.527965Z","shell.execute_reply.started":"2024-01-24T18:26:33.356071Z","shell.execute_reply":"2024-01-24T18:26:49.526951Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting numpy==1.22.0\n  Downloading numpy-1.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.24.3\n    Uninstalling numpy-1.24.3:\n      Successfully uninstalled numpy-1.24.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nchex 0.1.85 requires numpy>=1.24.1, but you have numpy 1.22.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ninequality 1.0.1 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nlibrosa 0.10.1 requires numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3, but you have numpy 1.22.0 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\nspglm 1.1.0 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspreg 1.4.2 requires numpy>=1.23, but you have numpy 1.22.0 which is incompatible.\ntensorflowjs 4.15.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.22.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoConfig\nimport transformers\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\nfrom torch import nn\nfrom datasets import Dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:26:49.529450Z","iopub.execute_input":"2024-01-24T18:26:49.529758Z","iopub.status.idle":"2024-01-24T18:27:06.712716Z","shell.execute_reply.started":"2024-01-24T18:26:49.529731Z","shell.execute_reply":"2024-01-24T18:27:06.711694Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Loading the teacher model (our finetuned full-size Llama 7B)","metadata":{}},{"cell_type":"code","source":"model_id = \"/kaggle/input/finetuned-llama/model_7_4Entries\"\nllama_base_id = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\ntokenizer = AutoTokenizer.from_pretrained(llama_base_id)\nteacher_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:27:06.716800Z","iopub.execute_input":"2024-01-24T18:27:06.717435Z","iopub.status.idle":"2024-01-24T18:30:38.528802Z","shell.execute_reply.started":"2024-01-24T18:27:06.717406Z","shell.execute_reply":"2024-01-24T18:30:38.527994Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"784e3e6adc524c709d002310b022d30a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:393: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:398: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Loading the student model (our 11-layer Llama 1.5B)\nNote: In our process here the extracted model or the extracted & already finetuned model can be loaded depending on the preferred order of training operations in our distil model pipeline","metadata":{}},{"cell_type":"code","source":"student_model = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/base-11-model/\")","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:30:38.530233Z","iopub.execute_input":"2024-01-24T18:30:38.530546Z","iopub.status.idle":"2024-01-24T18:31:05.811363Z","shell.execute_reply.started":"2024-01-24T18:30:38.530521Z","shell.execute_reply":"2024-01-24T18:31:05.810458Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM\n\nstudent_model = PeftModel.from_pretrained(student_model, \"/kaggle/input/distil-11-finetuned\")","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:31:05.812897Z","iopub.execute_input":"2024-01-24T18:31:05.813447Z","iopub.status.idle":"2024-01-24T18:31:06.506362Z","shell.execute_reply.started":"2024-01-24T18:31:05.813410Z","shell.execute_reply":"2024-01-24T18:31:06.505389Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# merging the lora matrices with the base weights\nstudent_model = student_model.merge_and_unload()","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:31:06.507705Z","iopub.execute_input":"2024-01-24T18:31:06.508019Z","iopub.status.idle":"2024-01-24T18:31:17.526194Z","shell.execute_reply.started":"2024-01-24T18:31:06.507993Z","shell.execute_reply":"2024-01-24T18:31:17.525409Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:257: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"student_model.model","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:31:17.527332Z","iopub.execute_input":"2024-01-24T18:31:17.527679Z","iopub.status.idle":"2024-01-24T18:31:17.536201Z","shell.execute_reply.started":"2024-01-24T18:31:17.527648Z","shell.execute_reply":"2024-01-24T18:31:17.535298Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"LlamaModel(\n  (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n  (layers): ModuleList(\n    (0-10): 11 x LlamaDecoderLayer(\n      (self_attn): LlamaAttention(\n        (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n        (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n        (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n        (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (mlp): LlamaMLP(\n        (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n        (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n        (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n        (act_fn): SiLU()\n      )\n      (input_layernorm): LlamaRMSNorm()\n      (post_attention_layernorm): LlamaRMSNorm()\n    )\n  )\n  (norm): LlamaRMSNorm()\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# *IMPORANT: For only loading the model and testing jump directly to model loading section from here*","metadata":{}},{"cell_type":"markdown","source":"##### Preparing student for QLoRa (also used in our distillation process)","metadata":{}},{"cell_type":"code","source":"student_model.gradient_checkpointing_enable()","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.570424Z","iopub.execute_input":"2024-01-24T14:46:10.570794Z","iopub.status.idle":"2024-01-24T14:46:10.578779Z","shell.execute_reply.started":"2024-01-24T14:46:10.570761Z","shell.execute_reply":"2024-01-24T14:46:10.577832Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**The code line below has been caused a fatal error for the custom DistillationTrainer defined later on regarding some \"inf checks ...\", exact causal relation could not have been determined due to time reasons, interesting subject for further theoretical investigation :)**","metadata":{}},{"cell_type":"code","source":"# from peft import prepare_model_for_kbit_training\n# student_model = prepare_model_for_kbit_training(student_model)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.579840Z","iopub.execute_input":"2024-01-24T14:46:10.580103Z","iopub.status.idle":"2024-01-24T14:46:10.589761Z","shell.execute_reply.started":"2024-01-24T14:46:10.580080Z","shell.execute_reply":"2024-01-24T14:46:10.588912Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.590924Z","iopub.execute_input":"2024-01-24T14:46:10.591271Z","iopub.status.idle":"2024-01-24T14:46:10.599872Z","shell.execute_reply.started":"2024-01-24T14:46:10.591238Z","shell.execute_reply":"2024-01-24T14:46:10.598933Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=8, \n    lora_alpha=32, \n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n    lora_dropout=0.05, \n    bias=\"none\", \n    task_type=\"CAUSAL_LM\",\n    inference_mode=False\n)\n\nstudent_model = get_peft_model(student_model, config)\nprint_trainable_parameters(student_model)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.601215Z","iopub.execute_input":"2024-01-24T14:46:10.601832Z","iopub.status.idle":"2024-01-24T14:46:10.768572Z","shell.execute_reply.started":"2024-01-24T14:46:10.601805Z","shell.execute_reply":"2024-01-24T14:46:10.767600Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"trainable params: 6871040 || all params: 1382172672 || trainable%: 0.4971187854595348\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Implementing custom trainer for distilloss calculation \n(code adapted from tiny-bert project https://github.com/philschmid/knowledge-distillation-transformers-pytorch-sagemaker/blob/master/knowledge-distillation.ipynb and significantly rewritten and adapted to fit our goal for Llama distillation)","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"CUDA is available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"CUDA is not available\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.769886Z","iopub.execute_input":"2024-01-24T14:46:10.770175Z","iopub.status.idle":"2024-01-24T14:46:10.775842Z","shell.execute_reply.started":"2024-01-24T14:46:10.770151Z","shell.execute_reply":"2024-01-24T14:46:10.774915Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"CUDA is available\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DistillationTrainingArguments(TrainingArguments):\n    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n        super().__init__(*args, **kwargs)\n        \n        self.alpha = alpha\n        self.temperature = temperature\n        \nclass DistillationTrainer(Trainer):\n    def __init__(self, *args, teacher_model=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.teacher = teacher_model\n        # place teacher on same device as student\n        #self._move_model_to_device(self.teacher,self.model.device)\n        self.teacher.eval()\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        ### trials from the debugging process, but not needed\n        #print(inputs)\n        # compute student output\n#         if hasattr(model, \"enable_input_require_grads\"):\n#             model.enable_input_require_grads()\n#         else:\n#             def make_inputs_require_grad(module, input, output):\n#                  output.requires_grad_(True)\n\n#             model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n\n        inputs = inputs.to(device)\n    \n        # requires_grad statements resulted from debugging process, probably mostly not needed \n        # also from a theoretical side this does probably not make sense totally\n        # further investigation not possible though because of Kaggle GPU limit reached\n        inputs.requires_grad=True\n        model.enable_input_require_grads()\n        model.requires_grad=True\n        \n        # student output\n        outputs_student = model.generate(input_ids = inputs.input_ids, max_length=200, output_scores=True,return_dict_in_generate=True)\n        outputs_student = outputs_student\n        student_logits = outputs_student.scores # tensor of probit tensors (for every output subword)\n        \n        # teacher output\n        outputs_teacher = self.teacher.generate(inputs.input_ids, max_length=200, output_scores=True,return_dict_in_generate=True)\n        outputs_teacher = outputs_teacher\n        teacher_logits = outputs_teacher.scores # tensor of probit tensors (for every output subword)\n        \n        \n        # Soften probabilities and compute distillation loss\n        loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n\n        total_loss = torch.zeros(1).to(device)\n        \n        # adding up loss over all subwords\n        for i in range(len(teacher_logits)):\n            current_loss = (\n                loss_function(\n                    F.softmax(teacher_logits[i].requires_grad_(True) / self.args.temperature, dim=-1),\n                    F.softmax(student_logits[i].requires_grad_(True) / self.args.temperature, dim=-1)) * (self.args.temperature ** 2)).to(device)\n            #print(current_loss)\n            current_loss = current_loss.reshape(1)\n            #print(current_loss)\n            \n            # new tensor to store the accumulated loss \n            # (not possible to overwrite because the tensor object requires grad and is tracked by torch)\n            new_total_loss = total_loss + current_loss\n\n            # updating the initial total_loss variable\n            total_loss = new_total_loss\n            \n        loss = total_loss.squeeze().float().abs() # .abs() for positive loss values instead of negative ones returned by custom loss fct\n        #print(loss)\n        return (loss, outputs_student) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.777418Z","iopub.execute_input":"2024-01-24T14:46:10.777782Z","iopub.status.idle":"2024-01-24T14:46:10.802227Z","shell.execute_reply.started":"2024-01-24T14:46:10.777749Z","shell.execute_reply":"2024-01-24T14:46:10.801321Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Defining training params and running training","metadata":{}},{"cell_type":"markdown","source":"#### dataset preprocessing","metadata":{}},{"cell_type":"markdown","source":"**Note: These dataset questions have been custom generated by utilizing a base Llama 7B prompted to generate general questions to a topic (for instance for Big Data), so in fact our proposed distillation process requires no ground data at all**","metadata":{}},{"cell_type":"code","source":"data_big_data = pd.read_csv(\"/kaggle/input/qa-distillation-set/qa_big_data.csv\")\ndata_ml_ds = pd.read_csv(\"/kaggle/input/qa-distillation-set/qa_ds_ml.csv\")\ndata_sql = pd.read_csv(\"/kaggle/input/qa-distillation-set/qa_sql.csv\")\ndata_sys = pd.read_csv(\"/kaggle/input/qa-distillation-set/qa_sys.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.803336Z","iopub.execute_input":"2024-01-24T14:46:10.803615Z","iopub.status.idle":"2024-01-24T14:46:10.840288Z","shell.execute_reply.started":"2024-01-24T14:46:10.803592Z","shell.execute_reply":"2024-01-24T14:46:10.839290Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"data_sql.rename(columns={'0': 'Question'}, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.841461Z","iopub.execute_input":"2024-01-24T14:46:10.841774Z","iopub.status.idle":"2024-01-24T14:46:10.852051Z","shell.execute_reply.started":"2024-01-24T14:46:10.841743Z","shell.execute_reply":"2024-01-24T14:46:10.851093Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"data_ml_ds.rename(columns={'question': 'Question'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.853271Z","iopub.execute_input":"2024-01-24T14:46:10.853622Z","iopub.status.idle":"2024-01-24T14:46:10.863314Z","shell.execute_reply.started":"2024-01-24T14:46:10.853593Z","shell.execute_reply":"2024-01-24T14:46:10.862429Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"data_sql.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.864447Z","iopub.execute_input":"2024-01-24T14:46:10.865282Z","iopub.status.idle":"2024-01-24T14:46:10.880844Z","shell.execute_reply.started":"2024-01-24T14:46:10.865246Z","shell.execute_reply":"2024-01-24T14:46:10.879836Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                           Question\n0           1  What is the difference between a table and a d...\n1           2  What is the purpose of a primary key in a rela...\n2           3  How does a foreign key work in a relational da...\n3           4  What is the difference between SQL and NoSQL d...\n4           5   What is the purpose of a database schema in SQL?","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Question</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>What is the difference between a table and a d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>What is the purpose of a primary key in a rela...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>How does a foreign key work in a relational da...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>What is the difference between SQL and NoSQL d...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>What is the purpose of a database schema in SQL?</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data = pd.concat([data_big_data, data_ml_ds, data_sql, data_sys], axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.882190Z","iopub.execute_input":"2024-01-24T14:46:10.882493Z","iopub.status.idle":"2024-01-24T14:46:10.888582Z","shell.execute_reply.started":"2024-01-24T14:46:10.882463Z","shell.execute_reply":"2024-01-24T14:46:10.887605Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"data = data.reset_index()","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.889792Z","iopub.execute_input":"2024-01-24T14:46:10.890090Z","iopub.status.idle":"2024-01-24T14:46:10.898207Z","shell.execute_reply.started":"2024-01-24T14:46:10.890067Z","shell.execute_reply":"2024-01-24T14:46:10.897386Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"data = data[\"Question\"]","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.899741Z","iopub.execute_input":"2024-01-24T14:46:10.900066Z","iopub.status.idle":"2024-01-24T14:46:10.908441Z","shell.execute_reply.started":"2024-01-24T14:46:10.900007Z","shell.execute_reply":"2024-01-24T14:46:10.907580Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"data = data.apply(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.909871Z","iopub.execute_input":"2024-01-24T14:46:10.910240Z","iopub.status.idle":"2024-01-24T14:46:10.938402Z","shell.execute_reply.started":"2024-01-24T14:46:10.910207Z","shell.execute_reply":"2024-01-24T14:46:10.937601Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"data = np.array(data)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.939640Z","iopub.execute_input":"2024-01-24T14:46:10.940036Z","iopub.status.idle":"2024-01-24T14:46:10.944727Z","shell.execute_reply.started":"2024-01-24T14:46:10.940000Z","shell.execute_reply":"2024-01-24T14:46:10.943882Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"**input transformed to embedding ids and mask (but mask is not there basically and will not further be used since learning takes place on the generation**","metadata":{}},{"cell_type":"code","source":"data[0]","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.945826Z","iopub.execute_input":"2024-01-24T14:46:10.946093Z","iopub.status.idle":"2024-01-24T14:46:10.957281Z","shell.execute_reply.started":"2024-01-24T14:46:10.946064Z","shell.execute_reply":"2024-01-24T14:46:10.956365Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [1, 1724, 338, 7997, 3630, 29973], 'attention_mask': [1, 1, 1, 1, 1, 1]}"},"metadata":{}}]},{"cell_type":"code","source":"len(data)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.958345Z","iopub.execute_input":"2024-01-24T14:46:10.958667Z","iopub.status.idle":"2024-01-24T14:46:10.967541Z","shell.execute_reply.started":"2024-01-24T14:46:10.958642Z","shell.execute_reply":"2024-01-24T14:46:10.966753Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"180"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndataloader = DataLoader(data, batch_size=2, shuffle=False)\n\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.968547Z","iopub.execute_input":"2024-01-24T14:46:10.969412Z","iopub.status.idle":"2024-01-24T14:46:10.976989Z","shell.execute_reply.started":"2024-01-24T14:46:10.969381Z","shell.execute_reply":"2024-01-24T14:46:10.976130Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# define training args\ntraining_args = DistillationTrainingArguments(\n    \n    # general training params\n    gradient_accumulation_steps=1,\n    warmup_steps=2,\n    num_train_epochs=0.4,\n    per_device_train_batch_size=2,\n    fp16=False,\n    learning_rate=2e-5,\n    seed=33,\n    output_dir=\"/kaggle/working/halfinput\",\n    optim=\"paged_adamw_8bit\",\n    logging_steps=1,\n\n    # distilation parameters\n    #alpha=0.5, # alpha not needed since we only use teacher soft targets for loss, so no weighting factor needed\n    temperature=3.0 # Hinton et. al. propose a high temperature for distillation\n    )","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.981599Z","iopub.execute_input":"2024-01-24T14:46:10.981973Z","iopub.status.idle":"2024-01-24T14:46:10.990468Z","shell.execute_reply.started":"2024-01-24T14:46:10.981948Z","shell.execute_reply":"2024-01-24T14:46:10.989703Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"trainer = DistillationTrainer(\n    student_model,\n    training_args,\n    teacher_model=teacher_model,\n    train_dataset=dataloader.dataset,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:10.991626Z","iopub.execute_input":"2024-01-24T14:46:10.991970Z","iopub.status.idle":"2024-01-24T14:46:11.574607Z","shell.execute_reply.started":"2024-01-24T14:46:10.991938Z","shell.execute_reply":"2024-01-24T14:46:11.573525Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport wandb\n\nuser_secrets = UserSecretsClient()\n\nmy_secret = user_secrets.get_secret(\"new_wandb\") \n\nwandb.login(key=my_secret)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:11.575797Z","iopub.execute_input":"2024-01-24T14:46:11.576105Z","iopub.status.idle":"2024-01-24T14:46:15.408866Z","shell.execute_reply.started":"2024-01-24T14:46:11.576062Z","shell.execute_reply":"2024-01-24T14:46:15.407827Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"#wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:15.410334Z","iopub.execute_input":"2024-01-24T14:46:15.411002Z","iopub.status.idle":"2024-01-24T14:46:15.415226Z","shell.execute_reply.started":"2024-01-24T14:46:15.410973Z","shell.execute_reply":"2024-01-24T14:46:15.414305Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-24T14:46:15.416705Z","iopub.execute_input":"2024-01-24T14:46:15.417042Z","iopub.status.idle":"2024-01-24T16:21:56.044546Z","shell.execute_reply.started":"2024-01-24T14:46:15.417017Z","shell.execute_reply":"2024-01-24T16:21:56.043504Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mw02marcus\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240124_144615-owyvorf9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/w02marcus/huggingface/runs/owyvorf9' target=\"_blank\">wandering-darkness-32</a></strong> to <a href='https://wandb.ai/w02marcus/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/w02marcus/huggingface' target=\"_blank\">https://wandb.ai/w02marcus/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/w02marcus/huggingface/runs/owyvorf9' target=\"_blank\">https://wandb.ai/w02marcus/huggingface/runs/owyvorf9</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1295: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [36/36 1:32:26, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>16828.027300</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>16831.107400</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>16336.202100</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>16626.230500</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>16622.238300</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>16720.441400</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>16902.898400</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>17246.716800</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>17050.134800</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>16813.089800</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>15749.014600</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>16540.630900</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>17247.980500</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>16587.896500</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>17171.537100</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>16691.404300</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>16971.726600</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>16633.132800</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>16787.396500</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>16421.115200</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>16713.017600</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>16195.589800</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>16638.998000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>16691.830100</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>16792.050800</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>16595.462900</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>16790.752000</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>16086.545900</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>16981.894500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>16548.212900</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>16876.939500</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>17130.960900</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>16732.738300</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>17070.203100</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>16453.562500</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>16770.271500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=36, training_loss=16717.99867078993, metrics={'train_runtime': 5740.2112, 'train_samples_per_second': 0.013, 'train_steps_per_second': 0.006, 'total_flos': 13986394791936.0, 'train_loss': 16717.99867078993, 'epoch': 0.4})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model(\"/kaggle/working/student_lora_adapters\")","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:34:02.941066Z","iopub.execute_input":"2024-01-24T16:34:02.941510Z","iopub.status.idle":"2024-01-24T16:34:03.054708Z","shell.execute_reply.started":"2024-01-24T16:34:02.941477Z","shell.execute_reply":"2024-01-24T16:34:03.053533Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## Model loading and testing","metadata":{}},{"cell_type":"markdown","source":"**Only run the cell below if no training has taken place in this run**","metadata":{}},{"cell_type":"markdown","source":"the distil-11-final dataset contains the lora matrices we got from the qa distillation process, those are loaded onto the unified model (base model + finetuning lora matrices)","metadata":{}},{"cell_type":"code","source":"student_model = PeftModel.from_pretrained(student_model, \"/kaggle/input/distil-11-final\")","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:33:33.423480Z","iopub.execute_input":"2024-01-24T18:33:33.423922Z","iopub.status.idle":"2024-01-24T18:33:33.953779Z","shell.execute_reply.started":"2024-01-24T18:33:33.423888Z","shell.execute_reply":"2024-01-24T18:33:33.952956Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation","metadata":{}},{"cell_type":"code","source":"pipeline_distil = transformers.pipeline(\n    \"text-generation\",\n    model=student_model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    tokenizer=tokenizer\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:33:35.991717Z","iopub.execute_input":"2024-01-24T18:33:35.992491Z","iopub.status.idle":"2024-01-24T18:33:35.998320Z","shell.execute_reply.started":"2024-01-24T18:33:35.992459Z","shell.execute_reply":"2024-01-24T18:33:35.997400Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","output_type":"stream"}]},{"cell_type":"code","source":"sequences1 = pipeline_distil(\n   f'[INST]At what point can we call something big data?[/INST]',\n   do_sample=True,\n   top_k=10,\n   num_return_sequences=1,\n   eos_token_id=tokenizer.eos_token_id,\n   max_length=200,\n)\n# sequences2 = pipeline_distil(\n#    f'[INST]Explain the RDBMS?[/INST]',\n#    do_sample=True,\n#    top_k=10,\n#    num_return_sequences=1,\n#    eos_token_id=tokenizer.eos_token_id,\n#    max_length=200,\n# )\n# sequences3 = pipeline_distil(\n#    f'[INST]What is Data Science?[/INST]',\n#    do_sample=True,\n#    top_k=10,\n#    num_return_sequences=1,\n#    eos_token_id=tokenizer.eos_token_id,\n#    max_length=200,\n# )","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:33:36.807066Z","iopub.execute_input":"2024-01-24T18:33:36.807984Z","iopub.status.idle":"2024-01-24T18:33:46.485575Z","shell.execute_reply.started":"2024-01-24T18:33:36.807950Z","shell.execute_reply":"2024-01-24T18:33:46.484740Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1295: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"sequences1","metadata":{"execution":{"iopub.status.busy":"2024-01-24T18:33:46.487018Z","iopub.execute_input":"2024-01-24T18:33:46.487326Z","iopub.status.idle":"2024-01-24T18:33:46.493167Z","shell.execute_reply.started":"2024-01-24T18:33:46.487300Z","shell.execute_reply":"2024-01-24T18:33:46.492225Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"[{'generated_text': '[INST]At what point can we call something big data?[/INST]The big data in the world nowaday holds this information: What are the data and what means are actually big data? Data was created after the 270 year programming. sierpion s values for the human data file 128482, with the next 8916 year with this data, the world is called \"Data in the world nowadays\". a database contains many more numbers than their numbers.  Data is stored for the future. This is one process called to release data from the entire universe. The entire database is stored under the entire view of the entire database, with its entire file stored in one full record. The world has been able to create an entire data value with a view of the world. Databases are now databases from the entire universe. The world is the world and all over there are. Datar Worlds have access to'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"## ======================== ##","metadata":{}},{"cell_type":"markdown","source":"### Trials for investigating the inner structure of Llama forward pass, inputs, generation function and huggingface adaptions of those functions ","metadata":{}},{"cell_type":"markdown","source":"### IMPORTANT: Code execution might or will fail in some cells below, this is intended, because this is just for documentation of the dev and debugging process","metadata":{}},{"cell_type":"markdown","source":"**this section documents all the trials made to generate insights and overcome certain issues**\n\nDuring the implementation of our custom trainer and the custom loss calculation several errors occured along the way. A couple of major important fixes have been added as comments along the way already.\n\nBecause we wanted to run a distillation without a ground dataset based only on a catalogue of questions, we had to redesign the compute_loss() function of the distillation trainer in a way that would allow us to generate whole answers in one pass and calculate the loss as a sum of differences between the subword probit tensors.\n\nSince at first we could not find a way to make the .generate() function also return logits, because the huggingface .generate() differs from the Llama GitHub defined .generate() we also spend some substiantal amount of time in investigating the forward pass logic of Llama to somehow find a way to implement the generation logic ourselfes, so to basically recursively to forward passes. This approach had some issues though and we could thankfully dismiss it because of the capability to output the logits directly from the .generate()","metadata":{}},{"cell_type":"markdown","source":"tokenizer investigation","metadata":{}},{"cell_type":"code","source":"torch.tensor(tokenizer(\"What is big data?\")[\"input_ids\"])","metadata":{"execution":{"iopub.status.busy":"2024-01-22T14:32:24.143584Z","iopub.execute_input":"2024-01-22T14:32:24.143933Z","iopub.status.idle":"2024-01-22T14:32:24.153272Z","shell.execute_reply.started":"2024-01-22T14:32:24.143908Z","shell.execute_reply":"2024-01-22T14:32:24.152139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_template = \"[INST]What is big data?[/INST]\"","metadata":{"execution":{"iopub.status.busy":"2024-01-23T09:33:59.748005Z","iopub.execute_input":"2024-01-23T09:33:59.748378Z","iopub.status.idle":"2024-01-23T09:33:59.754048Z","shell.execute_reply.started":"2024-01-23T09:33:59.748349Z","shell.execute_reply":"2024-01-23T09:33:59.753023Z"},"trusted":true},"execution_count":266,"outputs":[]},{"cell_type":"markdown","source":"investigating input processing to embedding tensors","metadata":{}},{"cell_type":"code","source":"input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()","metadata":{"execution":{"iopub.status.busy":"2024-01-23T09:33:59.992615Z","iopub.execute_input":"2024-01-23T09:33:59.992988Z","iopub.status.idle":"2024-01-23T09:34:00.001018Z","shell.execute_reply.started":"2024-01-23T09:33:59.992957Z","shell.execute_reply":"2024-01-23T09:33:59.999236Z"},"trusted":true},"execution_count":267,"outputs":[]},{"cell_type":"code","source":"input_ids","metadata":{"execution":{"iopub.status.busy":"2024-01-23T09:38:42.437440Z","iopub.execute_input":"2024-01-23T09:38:42.438249Z","iopub.status.idle":"2024-01-23T09:38:42.447622Z","shell.execute_reply.started":"2024-01-23T09:38:42.438214Z","shell.execute_reply":"2024-01-23T09:38:42.446419Z"},"trusted":true},"execution_count":275,"outputs":[{"execution_count":275,"output_type":"execute_result","data":{"text/plain":"tensor([[    1,   518, 25580, 29962,  5618,   338,  4802,   848, 29973, 29961,\n         29914, 25580, 29962]], device='cuda:0')"},"metadata":{}}]},{"cell_type":"markdown","source":"investigation of generation function","metadata":{}},{"cell_type":"code","source":"# this fails, because only the .generate() defined in the Llama 2 Github features logprobs=True\n# huggingface implementation features a standard .generate() method with different parameters\nteacher_model.generate(input_ids, logprobs=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T17:55:17.762627Z","iopub.execute_input":"2024-01-24T17:55:17.763316Z","iopub.status.idle":"2024-01-24T17:55:18.089086Z","shell.execute_reply.started":"2024-01-24T17:55:17.763281Z","shell.execute_reply":"2024-01-24T17:55:18.087856Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mteacher_model\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(input_ids, logprobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mNameError\u001b[0m: name 'teacher_model' is not defined"],"ename":"NameError","evalue":"name 'teacher_model' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# output_scores=True has been the final solution to get all the logits\noutput_teacher = teacher_model.generate(input_ids, max_length=200, output_scores=True,return_dict_in_generate=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T18:52:50.605389Z","iopub.execute_input":"2024-01-22T18:52:50.605764Z","iopub.status.idle":"2024-01-22T18:53:13.416305Z","shell.execute_reply.started":"2024-01-22T18:52:50.605732Z","shell.execute_reply":"2024-01-22T18:53:13.415474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_student.scores","metadata":{"execution":{"iopub.status.busy":"2024-01-22T19:29:32.342438Z","iopub.execute_input":"2024-01-22T19:29:32.343201Z","iopub.status.idle":"2024-01-22T19:29:32.498673Z","shell.execute_reply.started":"2024-01-22T19:29:32.343166Z","shell.execute_reply":"2024-01-22T19:29:32.497576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"shape investigation of logits returned","metadata":{}},{"cell_type":"code","source":"output_teacher.scores[0].shape","metadata":{"execution":{"iopub.status.busy":"2024-01-22T18:54:29.616661Z","iopub.execute_input":"2024-01-22T18:54:29.617498Z","iopub.status.idle":"2024-01-22T18:54:29.623208Z","shell.execute_reply.started":"2024-01-22T18:54:29.617464Z","shell.execute_reply":"2024-01-22T18:54:29.622204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"regenerating text out of the generation function outputs","metadata":{}},{"cell_type":"code","source":"generated_text = tokenizer.batch_decode(output_teacher, skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T14:50:17.047078Z","iopub.execute_input":"2024-01-22T14:50:17.047761Z","iopub.status.idle":"2024-01-22T14:50:17.055134Z","shell.execute_reply.started":"2024-01-22T14:50:17.047720Z","shell.execute_reply":"2024-01-22T14:50:17.053923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generated_text","metadata":{"execution":{"iopub.status.busy":"2024-01-22T14:50:23.984472Z","iopub.execute_input":"2024-01-22T14:50:23.984842Z","iopub.status.idle":"2024-01-22T14:50:23.992781Z","shell.execute_reply.started":"2024-01-22T14:50:23.984815Z","shell.execute_reply":"2024-01-22T14:50:23.991836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"student_model.generate(input_ids=input_ids, max_length=200)","metadata":{"execution":{"iopub.status.busy":"2024-01-23T09:35:01.108660Z","iopub.execute_input":"2024-01-23T09:35:01.109537Z","iopub.status.idle":"2024-01-23T09:35:37.956746Z","shell.execute_reply.started":"2024-01-23T09:35:01.109505Z","shell.execute_reply":"2024-01-23T09:35:37.955410Z"},"trusted":true},"execution_count":271,"outputs":[{"execution_count":271,"output_type":"execute_result","data":{"text/plain":"tensor([[    1,   518, 25580, 29962,  5618,   338,  4802,   848, 29973, 29961,\n         29914, 25580, 29962,  6970,   848,   338,   263,   376,  3752,   848,\n         29908,   376,  3752,   848, 29908,   376,  3752,   848, 29908,   376,\n          3752,   848, 29908,   376,  3752,   848, 29908,   376,  3752,   848,\n         29908,   376,  3752,   848, 29908,   376,  3752,   848, 29908,   376,\n          3752,   848, 29908,   376,  3752,   848, 29908,   376,  3752,   848,\n         29908,   376,  3752,   848, 29908,   376,  3752,   848, 29908,   376,\n          3752,   848, 29908,   376,  3752,   848, 29908,   376,  3752,   848,\n         29908,   376,  3752,   848, 29908,   376,  3752,   848, 29908,   376,\n          3752,   848, 29908,   376,  3752,   848, 29908,   376,  3752,   848,\n         29908,   376,  3752,   848, 29908,   376,  3752,   848, 29908,   376,\n          3752,   848, 29908,   376,  3752,   848, 29908,   376,  3752,   848,\n         29908,   376,  3752,   848, 29908,   376,  3752,   848, 29908,   376,\n          3752,   848, 29908,   376,  3752,   848, 29908,   376,  3752,   848,\n         29908,   376,  3752,   848, 29908,   376,  3752,   848, 29901, 29871,\n         29906, 29900, 29900, 29900, 29900, 29900, 29900, 29900, 29900, 29900,\n         29900, 29900, 29900, 29900, 29900, 29900, 29900, 29900, 29900, 29900,\n         29900, 29900, 29900, 29900, 29900, 29900, 29900, 29900, 29900, 29900,\n         29900, 29900, 29900, 29900, 29900, 29900, 29900, 29900, 29900, 29900,\n         29900, 29900, 29900, 29900, 29900, 29900, 29900, 29900, 29900, 29900]],\n       device='cuda:0')"},"metadata":{}}]},{"cell_type":"markdown","source":"forward pass investigation","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    result_teacher=teacher_model.forward(input_ids)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T17:52:44.969308Z","iopub.execute_input":"2024-01-22T17:52:44.970255Z","iopub.status.idle":"2024-01-22T17:52:45.399892Z","shell.execute_reply.started":"2024-01-22T17:52:44.970217Z","shell.execute_reply":"2024-01-22T17:52:45.399035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    result=student_model.forward(input_ids)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T16:39:17.721505Z","iopub.execute_input":"2024-01-22T16:39:17.721872Z","iopub.status.idle":"2024-01-22T16:39:17.818821Z","shell.execute_reply.started":"2024-01-22T16:39:17.721841Z","shell.execute_reply":"2024-01-22T16:39:17.817767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"teacher_probits = F.softmax(result_teacher.logits, dim=-1)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T17:54:46.606300Z","iopub.execute_input":"2024-01-22T17:54:46.606945Z","iopub.status.idle":"2024-01-22T17:54:46.611375Z","shell.execute_reply.started":"2024-01-22T17:54:46.606912Z","shell.execute_reply":"2024-01-22T17:54:46.610429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(torch.argmax(teacher_probits))","metadata":{"execution":{"iopub.status.busy":"2024-01-22T17:54:47.116957Z","iopub.execute_input":"2024-01-22T17:54:47.117667Z","iopub.status.idle":"2024-01-22T17:54:47.123819Z","shell.execute_reply.started":"2024-01-22T17:54:47.117633Z","shell.execute_reply":"2024-01-22T17:54:47.122874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(torch.argmax(torch.argmax(teacher_probits, dim=-1), dim=-1))","metadata":{"execution":{"iopub.status.busy":"2024-01-22T16:53:06.235786Z","iopub.execute_input":"2024-01-22T16:53:06.236178Z","iopub.status.idle":"2024-01-22T16:53:06.245447Z","shell.execute_reply.started":"2024-01-22T16:53:06.236135Z","shell.execute_reply":"2024-01-22T16:53:06.244468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concatenated_tensor = torch.cat((input_ids, new_tensor), 1)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T16:46:00.627509Z","iopub.execute_input":"2024-01-22T16:46:00.627886Z","iopub.status.idle":"2024-01-22T16:46:00.633994Z","shell.execute_reply.started":"2024-01-22T16:46:00.627855Z","shell.execute_reply":"2024-01-22T16:46:00.632995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.batch_decode(torch.argmax(teacher_probits, dim=-1))","metadata":{"execution":{"iopub.status.busy":"2024-01-22T16:46:36.910092Z","iopub.execute_input":"2024-01-22T16:46:36.911019Z","iopub.status.idle":"2024-01-22T16:46:36.919399Z","shell.execute_reply.started":"2024-01-22T16:46:36.910985Z","shell.execute_reply":"2024-01-22T16:46:36.918440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids","metadata":{"execution":{"iopub.status.busy":"2024-01-22T16:58:19.184514Z","iopub.execute_input":"2024-01-22T16:58:19.185337Z","iopub.status.idle":"2024-01-22T16:58:19.195421Z","shell.execute_reply.started":"2024-01-22T16:58:19.185307Z","shell.execute_reply":"2024-01-22T16:58:19.194447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_teacher=teacher_model.forward(input_ids)\nteacher_probits = F.softmax(result_teacher.logits / 2.0, dim=-1)\nnew_tensor = torch.argmax(teacher_probits, dim=-1)\nfor i in range(0,5):\n    with torch.no_grad():\n        result_teacher=teacher_model.forward(new_tensor)\n        teacher_probits = F.softmax(result_teacher.logits / 2.0, dim=-1)\n        new_tensor = torch.argmax(teacher_probits, dim=-1)\n        #concat_tensor = torch.cat((concat_tensor, new_tensor), 1)\n        print(tokenizer.batch_decode(new_tensor))","metadata":{"execution":{"iopub.status.busy":"2024-01-22T17:51:32.566789Z","iopub.execute_input":"2024-01-22T17:51:32.567684Z","iopub.status.idle":"2024-01-22T17:51:35.807258Z","shell.execute_reply.started":"2024-01-22T17:51:32.567648Z","shell.execute_reply":"2024-01-22T17:51:35.806326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.batch_decode(concat_tensor)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T17:01:22.062453Z","iopub.execute_input":"2024-01-22T17:01:22.062848Z","iopub.status.idle":"2024-01-22T17:01:22.071991Z","shell.execute_reply.started":"2024-01-22T17:01:22.062818Z","shell.execute_reply":"2024-01-22T17:01:22.070890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.logits","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:55:44.720728Z","iopub.execute_input":"2024-01-22T15:55:44.721783Z","iopub.status.idle":"2024-01-22T15:55:44.734077Z","shell.execute_reply.started":"2024-01-22T15:55:44.721747Z","shell.execute_reply":"2024-01-22T15:55:44.732933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"student_probits = F.log_softmax(result.logits / 2.0, dim=-1)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T16:41:29.704091Z","iopub.execute_input":"2024-01-22T16:41:29.704874Z","iopub.status.idle":"2024-01-22T16:41:29.710738Z","shell.execute_reply.started":"2024-01-22T16:41:29.704840Z","shell.execute_reply":"2024-01-22T16:41:29.709768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.batch_decode(torch.argmax(student_probits, dim=-1))","metadata":{"execution":{"iopub.status.busy":"2024-01-22T16:41:50.763246Z","iopub.execute_input":"2024-01-22T16:41:50.763869Z","iopub.status.idle":"2024-01-22T16:41:50.771838Z","shell.execute_reply.started":"2024-01-22T16:41:50.763833Z","shell.execute_reply":"2024-01-22T16:41:50.770819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_function = nn.KLDivLoss(reduction=\"batchmean\")","metadata":{"execution":{"iopub.status.busy":"2024-01-22T16:03:46.120026Z","iopub.execute_input":"2024-01-22T16:03:46.120417Z","iopub.status.idle":"2024-01-22T16:03:46.126816Z","shell.execute_reply.started":"2024-01-22T16:03:46.120387Z","shell.execute_reply":"2024-01-22T16:03:46.125456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_function(student_probits, teacher_probits)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T16:04:40.892969Z","iopub.execute_input":"2024-01-22T16:04:40.893802Z","iopub.status.idle":"2024-01-22T16:04:40.904860Z","shell.execute_reply.started":"2024-01-22T16:04:40.893767Z","shell.execute_reply":"2024-01-22T16:04:40.903753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generated_text_student = tokenizer.batch_decode(output_student, skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:53:15.043771Z","iopub.execute_input":"2024-01-22T15:53:15.044813Z","iopub.status.idle":"2024-01-22T15:53:15.054300Z","shell.execute_reply.started":"2024-01-22T15:53:15.044778Z","shell.execute_reply":"2024-01-22T15:53:15.053396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generated_text_student","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:53:15.871373Z","iopub.execute_input":"2024-01-22T15:53:15.872273Z","iopub.status.idle":"2024-01-22T15:53:15.879782Z","shell.execute_reply.started":"2024-01-22T15:53:15.872242Z","shell.execute_reply":"2024-01-22T15:53:15.878539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"student_model.generation_config","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:04:45.827128Z","iopub.execute_input":"2024-01-22T15:04:45.827745Z","iopub.status.idle":"2024-01-22T15:04:45.835960Z","shell.execute_reply.started":"2024-01-22T15:04:45.827710Z","shell.execute_reply":"2024-01-22T15:04:45.834749Z"},"trusted":true},"execution_count":null,"outputs":[]}]}