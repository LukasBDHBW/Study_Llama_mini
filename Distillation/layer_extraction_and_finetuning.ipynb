{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":7429546,"datasetId":4323443,"databundleVersionId":7521101},{"sourceType":"datasetVersion","sourceId":7455732,"datasetId":4328548,"databundleVersionId":7547692},{"sourceType":"datasetVersion","sourceId":7155103,"datasetId":4131944,"databundleVersionId":7244125},{"sourceType":"modelInstanceVersion","sourceId":4298,"databundleVersionId":6243355,"modelInstanceId":3093}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Layer extraction + finetuning of small Llama 1.4B ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-22T11:38:28.597430Z","iopub.execute_input":"2024-01-22T11:38:28.597664Z","iopub.status.idle":"2024-01-22T11:38:29.472112Z","shell.execute_reply.started":"2024-01-22T11:38:28.597642Z","shell.execute_reply":"2024-01-22T11:38:29.471164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git \n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q datasets","metadata":{"execution":{"iopub.status.busy":"2024-01-22T11:38:29.474163Z","iopub.execute_input":"2024-01-22T11:38:29.475029Z","iopub.status.idle":"2024-01-22T11:40:40.320447Z","shell.execute_reply.started":"2024-01-22T11:38:29.474995Z","shell.execute_reply":"2024-01-22T11:40:40.319106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install numpy==1.22.0","metadata":{"execution":{"iopub.status.busy":"2024-01-22T11:40:40.322086Z","iopub.execute_input":"2024-01-22T11:40:40.322498Z","iopub.status.idle":"2024-01-22T11:40:56.727091Z","shell.execute_reply.started":"2024-01-22T11:40:40.322460Z","shell.execute_reply":"2024-01-22T11:40:56.726017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Llama distillation","metadata":{}},{"cell_type":"markdown","source":"### loading model reduced to 11 hidden layers","metadata":{}},{"cell_type":"markdown","source":"**Note: Loading the model with fewer hidden layers does not give you an option to select the layers to keep, but loading the full model and cutting away some layers manually will throw index errors because layers try to access attention scores from previous layers**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoConfig\nimport transformers\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\nfrom torch import nn\nfrom datasets import Dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-22T11:40:56.730108Z","iopub.execute_input":"2024-01-22T11:40:56.730512Z","iopub.status.idle":"2024-01-22T11:41:13.043068Z","shell.execute_reply.started":"2024-01-22T11:40:56.730474Z","shell.execute_reply":"2024-01-22T11:41:13.042102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llama_config=AutoConfig.from_pretrained(\"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\")","metadata":{"execution":{"iopub.status.busy":"2024-01-22T11:41:13.044460Z","iopub.execute_input":"2024-01-22T11:41:13.045054Z","iopub.status.idle":"2024-01-22T11:41:13.056370Z","shell.execute_reply.started":"2024-01-22T11:41:13.045027Z","shell.execute_reply":"2024-01-22T11:41:13.055064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llama_config.num_hidden_layers=11","metadata":{"execution":{"iopub.status.busy":"2024-01-22T11:41:13.058010Z","iopub.execute_input":"2024-01-22T11:41:13.060473Z","iopub.status.idle":"2024-01-22T11:41:13.065672Z","shell.execute_reply.started":"2024-01-22T11:41:13.060426Z","shell.execute_reply":"2024-01-22T11:41:13.064832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nmodel_id = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel_first_11 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, config=llama_config)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-22T11:41:13.066616Z","iopub.execute_input":"2024-01-22T11:41:13.066879Z","iopub.status.idle":"2024-01-22T11:42:17.353671Z","shell.execute_reply.started":"2024-01-22T11:41:13.066856Z","shell.execute_reply":"2024-01-22T11:42:17.352742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading the full model (32 layers) and extracting wanted layers into custom ModuleList","metadata":{}},{"cell_type":"code","source":"model_id = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel_full = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-22T11:42:17.355033Z","iopub.execute_input":"2024-01-22T11:42:17.355356Z","iopub.status.idle":"2024-01-22T11:44:12.358176Z","shell.execute_reply.started":"2024-01-22T11:42:17.355329Z","shell.execute_reply":"2024-01-22T11:44:12.357360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test access\nmodel_full.model.layers[0]","metadata":{"execution":{"iopub.status.busy":"2024-01-22T11:44:12.359472Z","iopub.execute_input":"2024-01-22T11:44:12.359841Z","iopub.status.idle":"2024-01-22T11:44:12.367550Z","shell.execute_reply.started":"2024-01-22T11:44:12.359808Z","shell.execute_reply":"2024-01-22T11:44:12.366586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# == selecting distinct Llama layers (always attention + mlp + InputLayerNorm + PostAttentionLayerNorm) == #\n# defining a wrapper for all of the layers (to stricly follow Llama's layer tree hierarchies)\nlayers = nn.ModuleList()\n\n# adding layers for the distil model to the layers wrapper\ndistil_layers = [0,1,2,6,10,14,18,22,26,30,31]\nfor number, n in enumerate(distil_layers):\n    model_first_11.model.layers[number].load_state_dict(model_full.model.layers[n].state_dict())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-22T11:44:12.370593Z","iopub.execute_input":"2024-01-22T11:44:12.370870Z","iopub.status.idle":"2024-01-22T11:44:12.397564Z","shell.execute_reply.started":"2024-01-22T11:44:12.370847Z","shell.execute_reply":"2024-01-22T11:44:12.396729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Replacing the first 11 layers of the 11 layer config model with the custom layers","metadata":{}},{"cell_type":"markdown","source":"**Note: Doing this causes the same index out of range issue since the layer numbers have remained the same, so a renaming might be needed in the ModuleList**\n\n**Update: Renaming does not solve the issue, so probably each layer has its index hardcoded at some point and needs to access its initial number-1 proabably**\n\n**Solution idea: Keeping only the first 11 layers but replacing the weights with weights of other layers, this hopefully keeps numberings the same and still works**\n\n**Update: Works :)**","metadata":{}},{"cell_type":"code","source":"model_first_11.model.layers[4]","metadata":{"execution":{"iopub.status.busy":"2024-01-22T11:44:12.398654Z","iopub.execute_input":"2024-01-22T11:44:12.398989Z","iopub.status.idle":"2024-01-22T11:44:12.405165Z","shell.execute_reply.started":"2024-01-22T11:44:12.398957Z","shell.execute_reply":"2024-01-22T11:44:12.404236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Evaluation","metadata":{}},{"cell_type":"code","source":"pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_first_11,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    tokenizer=tokenizer\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-18T14:53:41.811569Z","iopub.execute_input":"2024-01-18T14:53:41.812411Z","iopub.status.idle":"2024-01-18T14:53:41.817434Z","shell.execute_reply.started":"2024-01-18T14:53:41.812373Z","shell.execute_reply":"2024-01-18T14:53:41.816288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences = pipeline(\n   '[INST]I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n[/INST]',\n   do_sample=True,\n   top_k=10,\n   num_return_sequences=1,\n   eos_token_id=tokenizer.eos_token_id,\n   max_length=400,\n)\nfor seq in sequences:\n   print(f\"Result: {seq['generated_text']}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-18T14:53:42.719493Z","iopub.execute_input":"2024-01-18T14:53:42.719974Z","iopub.status.idle":"2024-01-18T14:53:53.011063Z","shell.execute_reply.started":"2024-01-18T14:53:42.719933Z","shell.execute_reply":"2024-01-18T14:53:53.010089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## finetuning distilled model\n##### to hopefully establish meaningful connections between rearranged layers and regain ability of producing at least full words of finetuning domain as output","metadata":{}},{"cell_type":"markdown","source":"finetuning configuration","metadata":{}},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\n\nmodel_first_11.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model_first_11)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T21:35:33.792847Z","iopub.execute_input":"2024-01-21T21:35:33.793106Z","iopub.status.idle":"2024-01-21T21:35:33.854960Z","shell.execute_reply.started":"2024-01-21T21:35:33.793084Z","shell.execute_reply":"2024-01-21T21:35:33.854094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2024-01-21T21:35:33.855948Z","iopub.execute_input":"2024-01-21T21:35:33.856191Z","iopub.status.idle":"2024-01-21T21:35:33.861746Z","shell.execute_reply.started":"2024-01-21T21:35:33.856170Z","shell.execute_reply":"2024-01-21T21:35:33.860839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=8, \n    lora_alpha=32, \n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n    lora_dropout=0.05, \n    bias=\"none\", \n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, config)\nprint_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T21:35:33.862818Z","iopub.execute_input":"2024-01-21T21:35:33.863082Z","iopub.status.idle":"2024-01-21T21:35:34.035455Z","shell.execute_reply.started":"2024-01-21T21:35:33.863059Z","shell.execute_reply":"2024-01-21T21:35:34.034541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"input preparation","metadata":{}},{"cell_type":"code","source":"with open(\"/kaggle/input/edulllamafinetuning/Big Data - Pfisterer.txt\", 'r', encoding='utf-8') as file:\n        bigdata = file.read()\nsections = bigdata.split(\"Prof. Dr.-Ing. habil. Dennis Pfisterer\")\n\nimport re\ninp_bigdata= []\n\nfor section in sections: \n    i = 1\n    if section.startswith(\"\\n\"+str(i)):\n        text_parts = section\n        try:\n            while text_parts != None:\n                text_parts = text_parts.split(\"\\n\"+str(i+1),1)\n                inp_bigdata.append(text_parts[0].replace(\"\\n\",\" \"))\n                text_parts = text_parts[1]\n                i += 1\n        except:\n            text_parts = None\n            \n#inp_bigdata","metadata":{"execution":{"iopub.status.busy":"2024-01-21T21:35:34.036697Z","iopub.execute_input":"2024-01-21T21:35:34.037173Z","iopub.status.idle":"2024-01-21T21:35:34.054668Z","shell.execute_reply.started":"2024-01-21T21:35:34.037140Z","shell.execute_reply":"2024-01-21T21:35:34.053963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/input/edulllamafinetuning/Datenbanken.txt\", 'r', encoding='utf-8') as file:\n        databases = file.read()\n\nimport re\n\ndef split_text_on_numbers(text):\n    # Verwenden Sie ein reguläres Ausdrucksmuster, um den Text zu splitten\n    split_sections = re.split(r'\\n\\d+\\n', text)\n\n    # Entfernen von Leerzeichen und leeren Zeichenketten\n    split_sections = [s.strip().replace(\"\\n\",\" \").replace(\"–\\xa0\",\"\")for s in split_sections if s.strip()]\n\n    return split_sections\n\ninp_database = split_text_on_numbers(databases)\n#inp_database","metadata":{"execution":{"iopub.status.busy":"2024-01-21T21:35:34.055908Z","iopub.execute_input":"2024-01-21T21:35:34.056201Z","iopub.status.idle":"2024-01-21T21:35:34.072269Z","shell.execute_reply.started":"2024-01-21T21:35:34.056175Z","shell.execute_reply":"2024-01-21T21:35:34.071597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/input/edulllamafinetuning/Introduction_to_Data_Science_eng.txt\", 'r', encoding='utf-8') as file:\n        ds = file.read()\n\nimport re\n\ndef split_text_on_name(text):\n    # Verwenden Sie ein reguläres Ausdrucksmuster, das verschiedene Formate des Namens abdeckt\n    #split_sections = re.split(r'Prof\\.?\\s*Dr\\.?\\s*Bernhard Drabant', text)\n    split_sections = re.split(r'Bernhard Drabant', text)\n\n    # Entfernen von Leerzeichen und leeren Zeichenketten\n    split_sections = [s.strip().replace(\"\\n\", \" \").replace(\"\\x0c\",\"\") for s in split_sections if s.strip()]\n\n    return split_sections\n\ninp_ds = split_text_on_name(ds)\n#inp_ds","metadata":{"execution":{"iopub.status.busy":"2024-01-21T21:35:34.073219Z","iopub.execute_input":"2024-01-21T21:35:34.073512Z","iopub.status.idle":"2024-01-21T21:35:34.085719Z","shell.execute_reply.started":"2024-01-21T21:35:34.073488Z","shell.execute_reply":"2024-01-21T21:35:34.084907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/input/edulllamafinetuning/Kommunikations_und_Betriebssysteme_eng.txt\",'r', encoding='utf-8') as file:\n        kub = file.read()\n\nimport re\n\ndef merge_items_with_previous(lst):\n    merged_list = []\n    current_item = ''\n\n    for item in lst:\n        # Check if the item starts with a lowercase letter or a special character\n        if item and (item[0].islower() or not item[0].isalnum()):\n            # Merge with the previous item\n            current_item += ' ' + item\n        else:\n            # Add the current merged item to the list\n            if current_item:\n                merged_list.append(current_item.strip())\n            # Set the current item to the current item without merging\n            current_item = item\n\n    # Add the last merged item to the list\n    if current_item:\n        merged_list.append(current_item.strip())\n\n    return merged_list\n\nsplit_sections = re.split(r'Course: WWI21DSB', kub)\ninp_KuB = []\n\nfor index, section in enumerate(split_sections):\n    match index:\n        case 0:\n                continue\n        case 1: \n            section = section.split(\"\\n\")[58:]\n        case _: \n            section = section.split(\"\\n\")[13:]\n\n    \n    sec_result = merge_items_with_previous(section)\n    inp_KuB += sec_result\n\ndef filter_items_with_one_word(lst):\n    return [item for item in lst if len(item.split()) > 1]\n\ninp_KuB = filter_items_with_one_word(inp_KuB) # removed 1000 single words \n#inp_KuB","metadata":{"execution":{"iopub.status.busy":"2024-01-21T21:35:34.086714Z","iopub.execute_input":"2024-01-21T21:35:34.087006Z","iopub.status.idle":"2024-01-21T21:35:34.109085Z","shell.execute_reply.started":"2024-01-21T21:35:34.086981Z","shell.execute_reply":"2024-01-21T21:35:34.108397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/input/edulllamafinetuning/ML_fundamentals.txt\", 'r', encoding='utf-8') as file:\n        ml = file.read()\n\nimport re\n\ndef split_text_on_name(text):\n    # Use a regular expression pattern that covers various formats of the name\n    split_sections = re.split(r'Daniel Wehner, M.Sc. \\(SAP SE\\), Winter term 2023/2024', text)\n\n    # Remove spaces and empty strings\n    split_sections = [s.strip().replace(\"\\x0c\",\"\") for s in split_sections if s.strip()]\n\n    truncated_sections = []\n    for section in split_sections:\n        *_,truncated_section = re.split(r'\\n\\n', section, 1)\n        truncated_sections.append(truncated_section.strip().replace(\"\\n\",\" \"))\n\n    return truncated_sections\n\ninp_ml = split_text_on_name(ml)\n#inp_ml","metadata":{"execution":{"iopub.status.busy":"2024-01-21T21:35:34.110062Z","iopub.execute_input":"2024-01-21T21:35:34.110342Z","iopub.status.idle":"2024-01-21T21:35:34.127086Z","shell.execute_reply.started":"2024-01-21T21:35:34.110315Z","shell.execute_reply":"2024-01-21T21:35:34.126412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final = inp_bigdata+inp_database+inp_ds+inp_KuB+inp_ml\nlen(final)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T21:35:34.128029Z","iopub.execute_input":"2024-01-21T21:35:34.128306Z","iopub.status.idle":"2024-01-21T21:35:34.134222Z","shell.execute_reply.started":"2024-01-21T21:35:34.128260Z","shell.execute_reply":"2024-01-21T21:35:34.133320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_lengths = [len(str(text)) for text in final]\n\n# Calculate the average length\naverage_length = sum(text_lengths) / len(text_lengths)\naverage_length","metadata":{"execution":{"iopub.status.busy":"2024-01-21T21:35:34.135423Z","iopub.execute_input":"2024-01-21T21:35:34.135829Z","iopub.status.idle":"2024-01-21T21:35:34.143563Z","shell.execute_reply.started":"2024-01-21T21:35:34.135798Z","shell.execute_reply":"2024-01-21T21:35:34.142762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split_entries = []\n\nfor text in final:\n    # Check if the number of words in the text is greater than 600\n    if len(text.split()) > 600:\n        # Splitting the text into parts of average_length words\n        word_parts = [\" \".join(text.split()[i:i + int(average_length)]) for i in range(0, len(text.split()), int(average_length))]\n        # Extend the split_entries list with the word_parts\n        split_entries.extend(word_parts)\n    else:\n        # If the text has 600 or fewer words, add the entire text to split_entries\n        split_entries += [text]\n    \nlen(split_entries)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T21:35:34.144682Z","iopub.execute_input":"2024-01-21T21:35:34.145240Z","iopub.status.idle":"2024-01-21T21:35:34.172258Z","shell.execute_reply.started":"2024-01-21T21:35:34.145208Z","shell.execute_reply":"2024-01-21T21:35:34.171327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loonger_entries = []\n\ndef combine_entries(a,b,c,d):\n    combined = a +\" \"+ b +\" \"+ c +\" \"+ d \n    return combined\n\nfor index in range(1,len(split_entries),4):\n    inputs = combine_entries(split_entries[index-3],split_entries[index-2],split_entries[index-1],split_entries[index])\n    loonger_entries.append(inputs)\n\n#loonger_entries","metadata":{"execution":{"iopub.status.busy":"2024-01-21T21:35:34.173377Z","iopub.execute_input":"2024-01-21T21:35:34.173959Z","iopub.status.idle":"2024-01-21T21:35:34.182448Z","shell.execute_reply.started":"2024-01-21T21:35:34.173928Z","shell.execute_reply":"2024-01-21T21:35:34.181589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dict = {\"text\":loonger_entries}\ndata = Dataset.from_dict(data_dict)\ndata = data.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)\ndata","metadata":{"execution":{"iopub.status.busy":"2024-01-21T21:35:34.183479Z","iopub.execute_input":"2024-01-21T21:35:34.183737Z","iopub.status.idle":"2024-01-21T21:35:34.642653Z","shell.execute_reply.started":"2024-01-21T21:35:34.183715Z","shell.execute_reply":"2024-01-21T21:35:34.641773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"finetuning training process","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport wandb\n\nuser_secrets = UserSecretsClient()\n\nmy_secret = user_secrets.get_secret(\"new_wandb\") \n\nwandb.login(key=my_secret)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T21:35:34.643843Z","iopub.execute_input":"2024-01-21T21:35:34.644134Z","iopub.status.idle":"2024-01-21T21:35:37.647558Z","shell.execute_reply.started":"2024-01-21T21:35:34.644108Z","shell.execute_reply":"2024-01-21T21:35:37.646673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### === huggingface saving has been tried for saving the whole model in one go, but has been dismissed again ===\n# from huggingface_hub import notebook_login, login\n# from huggingface_hub import HfFolder\n# repo_name = \"Marcus02W/extracted-llama-finetuned\"","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:57:49.461809Z","iopub.execute_input":"2024-01-24T16:57:49.462194Z","iopub.status.idle":"2024-01-24T16:57:49.490611Z","shell.execute_reply.started":"2024-01-24T16:57:49.462162Z","shell.execute_reply":"2024-01-24T16:57:49.489433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# login(user_secrets.get_secret(\"Llama_pushing\"))","metadata":{"execution":{"iopub.status.busy":"2024-01-21T22:28:37.249273Z","iopub.execute_input":"2024-01-21T22:28:37.249701Z","iopub.status.idle":"2024-01-21T22:28:37.254348Z","shell.execute_reply.started":"2024-01-21T22:28:37.249672Z","shell.execute_reply":"2024-01-21T22:28:37.253289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:04:07.021393Z","iopub.execute_input":"2024-01-21T15:04:07.022078Z","iopub.status.idle":"2024-01-21T15:04:07.026124Z","shell.execute_reply.started":"2024-01-21T15:04:07.022050Z","shell.execute_reply":"2024-01-21T15:04:07.025284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom torch.utils.data import DataLoader\n\ndataloader = DataLoader(data, batch_size=1, shuffle=False)\n\ntokenizer.pad_token = tokenizer.eos_token\n\nepochen = 7\n\ntrainer = transformers.Trainer(\n    model=model,\n    #train_dataset=data,\n    train_dataset=dataloader.dataset,\n    args=transformers.TrainingArguments(\n        num_train_epochs=epochen,\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=3,\n        warmup_steps=2,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=1,\n        output_dir=\"/kaggle/working/halfinput\",\n        optim=\"paged_adamw_8bit\",\n#         # push to hub parameters\n#         push_to_hub=True,\n#         hub_strategy=\"every_save\",\n#         hub_model_id=repo_name,\n#         hub_token=HfFolder.get_token()\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n)\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-21T21:44:35.765515Z","iopub.execute_input":"2024-01-21T21:44:35.765895Z","iopub.status.idle":"2024-01-21T21:47:28.391649Z","shell.execute_reply.started":"2024-01-21T21:44:35.765867Z","shell.execute_reply":"2024-01-21T21:47:28.390520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model(\"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2024-01-21T22:27:00.441082Z","iopub.execute_input":"2024-01-21T22:27:00.442093Z","iopub.status.idle":"2024-01-21T22:27:00.446153Z","shell.execute_reply.started":"2024-01-21T22:27:00.442055Z","shell.execute_reply":"2024-01-21T22:27:00.445153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_first_11.generation_config.do_sample=True","metadata":{"execution":{"iopub.status.busy":"2024-01-22T11:44:12.406459Z","iopub.execute_input":"2024-01-22T11:44:12.407129Z","iopub.status.idle":"2024-01-22T11:44:12.413518Z","shell.execute_reply.started":"2024-01-22T11:44:12.407104Z","shell.execute_reply":"2024-01-22T11:44:12.412691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_first_11.save_pretrained(\"/kaggle/working/full_model\")","metadata":{"execution":{"iopub.status.busy":"2024-01-22T11:44:12.414583Z","iopub.execute_input":"2024-01-22T11:44:12.414877Z","iopub.status.idle":"2024-01-22T11:44:16.170366Z","shell.execute_reply.started":"2024-01-22T11:44:12.414854Z","shell.execute_reply":"2024-01-22T11:44:16.169370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline_distil = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    tokenizer=tokenizer\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences1 = pipeline_distil(\n   f'[INST]What is big data?[/INST]',\n   do_sample=True,\n   top_k=10,\n   num_return_sequences=1,\n   eos_token_id=tokenizer.eos_token_id,\n   max_length=200,\n)\nsequences2 = pipeline_distil(\n   f'[INST]Explain the RDBMS?[/INST]',\n   do_sample=True,\n   top_k=10,\n   num_return_sequences=1,\n   eos_token_id=tokenizer.eos_token_id,\n   max_length=200,\n)\nsequences3 = pipeline_distil(\n   f'[INST]What is Data Science?[/INST]',\n   do_sample=True,\n   top_k=10,\n   num_return_sequences=1,\n   eos_token_id=tokenizer.eos_token_id,\n   max_length=200,\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences3","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model loading from datasets","metadata":{}},{"cell_type":"markdown","source":"**Note: Calling trainer.save after QLoRa finetuning does only saft the adapter matrices, however this causes problems since when just loading that, the base model with 32 layers will be automatically used again. Therefore we saved the 11 layer model with the custom weights (extracted from defined layers) as well and changed the base model path in the config json to the path of the 11-layer model dataset.**","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM\n\n#config = PeftConfig.from_pretrained(\"/kaggle/input/distil-11-finetuned\")\nmodel = AutoModelForCausalLM.from_pretrained(\"/kaggle/working/full_model\")\nmodel = PeftModel.from_pretrained(model, \"/kaggle/input/distil-11-finetuned\")","metadata":{"execution":{"iopub.status.busy":"2024-01-22T11:44:16.171745Z","iopub.execute_input":"2024-01-22T11:44:16.172144Z","iopub.status.idle":"2024-01-22T11:44:17.876209Z","shell.execute_reply.started":"2024-01-22T11:44:16.172107Z","shell.execute_reply":"2024-01-22T11:44:17.875199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline2 = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    tokenizer=tokenizer\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T11:44:17.877616Z","iopub.execute_input":"2024-01-22T11:44:17.877990Z","iopub.status.idle":"2024-01-22T11:44:17.884066Z","shell.execute_reply.started":"2024-01-22T11:44:17.877956Z","shell.execute_reply":"2024-01-22T11:44:17.883050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences2 = pipeline2(\n   f'[INST]What is Big Data?[/INST]',\n   do_sample=True,\n   top_k=10,\n   num_return_sequences=1,\n   eos_token_id=tokenizer.eos_token_id,\n   max_length=200,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T11:47:59.295407Z","iopub.execute_input":"2024-01-22T11:47:59.295838Z","iopub.status.idle":"2024-01-22T11:48:07.671763Z","shell.execute_reply.started":"2024-01-22T11:47:59.295806Z","shell.execute_reply":"2024-01-22T11:48:07.670645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences2","metadata":{"execution":{"iopub.status.busy":"2024-01-22T11:48:07.673603Z","iopub.execute_input":"2024-01-22T11:48:07.674363Z","iopub.status.idle":"2024-01-22T11:48:07.680606Z","shell.execute_reply.started":"2024-01-22T11:48:07.674329Z","shell.execute_reply":"2024-01-22T11:48:07.679629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}