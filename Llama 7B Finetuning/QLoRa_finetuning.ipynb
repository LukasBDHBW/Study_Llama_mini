{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7155103,"sourceType":"datasetVersion","datasetId":4131944},{"sourceId":4298,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3093}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Finetuning of Llama 7B on lecture slides","metadata":{}},{"cell_type":"markdown","source":"QLoRa finetuning section code used from (and adapted to Llama):\nhttps://huggingface.co/blog/4bit-transformers-bitsandbytes\nhttps://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-12T12:08:22.621726Z","iopub.execute_input":"2023-12-12T12:08:22.622400Z","iopub.status.idle":"2023-12-12T12:08:22.979852Z","shell.execute_reply.started":"2023-12-12T12:08:22.622372Z","shell.execute_reply":"2023-12-12T12:08:22.978943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git \n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q datasets","metadata":{"execution":{"iopub.status.busy":"2023-12-12T12:08:22.981901Z","iopub.execute_input":"2023-12-12T12:08:22.982285Z","iopub.status.idle":"2023-12-12T12:10:30.630047Z","shell.execute_reply.started":"2023-12-12T12:08:22.982258Z","shell.execute_reply":"2023-12-12T12:10:30.628788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install numpy==1.22.0","metadata":{"execution":{"iopub.status.busy":"2023-12-12T12:10:30.631575Z","iopub.execute_input":"2023-12-12T12:10:30.631912Z","iopub.status.idle":"2023-12-12T12:10:48.303010Z","shell.execute_reply.started":"2023-12-12T12:10:30.631883Z","shell.execute_reply":"2023-12-12T12:10:48.301885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nmodel_id = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    tokenizer=tokenizer\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T12:10:48.305801Z","iopub.execute_input":"2023-12-12T12:10:48.306225Z","iopub.status.idle":"2023-12-12T12:13:54.622641Z","shell.execute_reply.started":"2023-12-12T12:10:48.306186Z","shell.execute_reply":"2023-12-12T12:13:54.621863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences = pipeline(\n    '[INST]I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n[/INST]',\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    max_length=200,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-12T12:13:54.623726Z","iopub.execute_input":"2023-12-12T12:13:54.624257Z","iopub.status.idle":"2023-12-12T12:14:12.218586Z","shell.execute_reply.started":"2023-12-12T12:13:54.624231Z","shell.execute_reply":"2023-12-12T12:14:12.217517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences = pipeline(\n    '[INST]I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n[INST]',\n    do_sample=False,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    max_length=200,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-12T12:14:12.219879Z","iopub.execute_input":"2023-12-12T12:14:12.220199Z","iopub.status.idle":"2023-12-12T12:14:25.407557Z","shell.execute_reply.started":"2023-12-12T12:14:12.220173Z","shell.execute_reply":"2023-12-12T12:14:25.406573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences = pipeline(\n    '[INST]What is Big Data?[/INST]',\n    do_sample=False,\n    #top_k=2,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    max_length=200,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-12T12:14:25.409001Z","iopub.execute_input":"2023-12-12T12:14:25.409674Z","iopub.status.idle":"2023-12-12T12:14:40.031155Z","shell.execute_reply.started":"2023-12-12T12:14:25.409639Z","shell.execute_reply":"2023-12-12T12:14:40.030221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### finetunig","metadata":{}},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T12:14:40.032356Z","iopub.execute_input":"2023-12-12T12:14:40.032677Z","iopub.status.idle":"2023-12-12T12:14:40.087600Z","shell.execute_reply.started":"2023-12-12T12:14:40.032650Z","shell.execute_reply":"2023-12-12T12:14:40.086865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2023-12-12T12:14:40.088661Z","iopub.execute_input":"2023-12-12T12:14:40.088966Z","iopub.status.idle":"2023-12-12T12:14:40.094763Z","shell.execute_reply.started":"2023-12-12T12:14:40.088942Z","shell.execute_reply":"2023-12-12T12:14:40.093835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=8, \n    lora_alpha=32, \n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n    lora_dropout=0.05, \n    bias=\"none\", \n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, config)\nprint_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T12:14:40.097404Z","iopub.execute_input":"2023-12-12T12:14:40.097686Z","iopub.status.idle":"2023-12-12T12:14:40.562406Z","shell.execute_reply.started":"2023-12-12T12:14:40.097662Z","shell.execute_reply":"2023-12-12T12:14:40.561282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndata = load_dataset('/kaggle/input/edulllamafinetuning')\ndata = data.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T12:14:40.563500Z","iopub.execute_input":"2023-12-12T12:14:40.563789Z","iopub.status.idle":"2023-12-12T12:14:42.888218Z","shell.execute_reply.started":"2023-12-12T12:14:40.563764Z","shell.execute_reply":"2023-12-12T12:14:42.887460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### finetuning training process","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport wandb\n\nuser_secrets = UserSecretsClient()\n\nmy_secret = user_secrets.get_secret(\"wandb_api_key\") \n\nwandb.login(key=my_secret)","metadata":{"execution":{"iopub.status.busy":"2023-12-12T12:31:16.887756Z","iopub.execute_input":"2023-12-12T12:31:16.888758Z","iopub.status.idle":"2023-12-12T12:31:17.699307Z","shell.execute_reply.started":"2023-12-12T12:31:16.888723Z","shell.execute_reply":"2023-12-12T12:31:17.698224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\n\ntokenizer.pad_token = tokenizer.eos_token\n\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=data[\"train\"],\n    args=transformers.TrainingArguments(\n        num_train_epochs=1,\n        per_device_train_batch_size=20,\n        gradient_accumulation_steps=2,\n        warmup_steps=2,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=1,\n        output_dir=\"/kaggle/working/\",\n        optim=\"paged_adamw_8bit\"\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-12T12:14:42.889520Z","iopub.execute_input":"2023-12-12T12:14:42.890453Z","iopub.status.idle":"2023-12-12T12:16:27.555031Z","shell.execute_reply.started":"2023-12-12T12:14:42.890414Z","shell.execute_reply":"2023-12-12T12:16:27.553408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model(\"/kaggle/working/\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}